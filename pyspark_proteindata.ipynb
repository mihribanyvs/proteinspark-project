{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0221e933-ac53-4274-baef-0600c23f09e9",
   "metadata": {},
   "source": [
    "### Embedding and Cosine Similarity\n",
    "\n",
    "In this section we are collecting the sequences to embed them using a Pretrained model from Rostlab's [ProtTrans](https://github.com/agemagician/ProtTrans). After this embedding the cosine similarity of each embedded protein sequence is achieved and graphed as a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00b87ec7-2990-4bb8-a637-1c5c6f7e3a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BioPython library for collecting the sequences from cif files\n",
    "from Bio.PDB import PDBList\n",
    "from Bio.PDB.MMCIFParser import MMCIFParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69cf507f-7791-4b9b-899b-0433a31fa609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation libraries\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import ArrayType, DoubleType, MapType\n",
    "from pyspark.ml.linalg import DenseVector, VectorUDT\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "700fe0d1-c9ef-41b3-b363-961b9fe463b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/27 11:33:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Creating the spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://master:7077\")\\\n",
    "    .appName(\"Proteindata spark application\")\\\n",
    "    .config(\"spark.executor.memory\", \"4096m\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c1c570a-199d-4c57-b6d7-2173f3c763aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the spark context\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0654db69-75b0-466d-b46b-a9cdfb0fa4ae",
   "metadata": {},
   "source": [
    "### Getting the file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66a6ea2c-43a1-4fcd-be8c-5cc26a6df9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the file paths\n",
    "base_path = '/data_files/cif_files'\n",
    "base_path_edit = '/data_files/cif_files/{}'\n",
    "file_names = os.listdir(base_path)\n",
    "file_list = [base_path_edit.format(i) for i in file_names]\n",
    "files_rdd = sc.parallelize(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c16b181-ef99-4972-8f03-188894c60d34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1712"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting all of the paths to see if there are any errors\n",
    "files_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cafcc4-1a5b-49ec-8425-b2524ebfe077",
   "metadata": {},
   "source": [
    "### Parsing the file\n",
    "\n",
    "#### Parsing the files using the Biopython library to get the sequence. The output of this function is the id of the protein, sequence and the length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de1a7fe6-05a7-487c-a79c-0892a18fd302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file(file):\n",
    "\n",
    "    cif_parser = MMCIFParser(QUIET=True) # CIF file parser\n",
    "    length = 0 # Setting the length initially to 0 for error correction\n",
    "    name = file.split('/')[3].split('.')[0] # Getting the id of the protein\n",
    "    structure = cif_parser.get_structure(\"protein\", file) # getting structure ? try \"protein\"\n",
    "\n",
    "    # Dictionary for residue names\n",
    "    d3to1 = {'CYS': 'C', 'ASP': 'D', 'SER': 'S', 'GLN': 'Q', 'LYS': 'K',\n",
    "    'ILE': 'I', 'PRO': 'P', 'THR': 'T', 'PHE': 'F', 'ASN': 'N',\n",
    "    'GLY': 'G', 'HIS': 'H', 'LEU': 'L', 'ARG': 'R', 'TRP': 'W',\n",
    "    'ALA': 'A', 'VAL':'V', 'GLU': 'E', 'TYR': 'Y', 'MET': 'M'}\n",
    "\n",
    "    # The output of the cif parser needs to be looped in order to get the sequence itself    \n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            sequence = [d3to1.get(residue.get_resname(), 'X') for residue in chain.get_residues()]\n",
    "            length = len(sequence)\n",
    "    \n",
    "    return name,sequence,length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6121fa2a-1c87-4f9c-9ef6-8658be829e52",
   "metadata": {},
   "source": [
    "### Creating a dataframe \n",
    "#### A dataframe containing the id, length and the token of the each sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d984795-af10-4d5e-ba73-a4859a88a078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an RDD for the tokens\n",
    "def tokens_df_creator(file_path):\n",
    "\n",
    "    data = []\n",
    "    name, sequence, length = parse_file(file_path)\n",
    "    row_value = {\n",
    "        'id':name,\n",
    "        'length':length,\n",
    "        'tokens':sequence,\n",
    "    }\n",
    "    #if 32 <= row_value['length'] <= 256:\n",
    "    data.append(Row(**row_value))\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Turning the RDD into a DF for easier usage\n",
    "tokens_rdd = files_rdd.flatMap(tokens_df_creator) # FlatMap applied to the RDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a0cb23b-e607-4e9c-8a18-0ddbe635d86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tokens_df = tokens_rdd.toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92198f8-0468-448d-98b9-98071c3a23cc",
   "metadata": {},
   "source": [
    "#### Checking output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d70ed89-ee64-4a40-aaa0-7b1f83c34f4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokens_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232aae94-6aca-495f-af07-00184aebea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_df.where(tokens_df.length==0).show()\n",
    "# running twice showString at NativeMethodAccessorImpl.java:0\n",
    "#8 minutes\n",
    "#12 minutes ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018b4aad-11d7-4610-9f5b-dd0a43aa25e8",
   "metadata": {},
   "source": [
    "### Frequency analysis for every sequence\n",
    "\n",
    "#### Here I am going to create a dataframe consisting of the frequency analysis of each sequence. The dataframe will consist of columns as, sequence id, most frequent amino acid (mf_aa), most frequent amino acid percentage (mf_aa_freq)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ecb63d-aa53-4459-b8f5-e155b1060eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a10a082-bc08-4ccc-9624-3fa22bb0a043",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = (\n",
    "    tokens_df\n",
    "    .withColumn(\"Dist\", F.array_distinct(\"Tokens\"))  # Get distinct tokens for the current sequence\n",
    "    .withColumn(\n",
    "        \"Counts\",\n",
    "        F.expr(\n",
    "            \"\"\"\n",
    "            transform(\n",
    "                Dist,\n",
    "                x -> aggregate(\n",
    "                    Tokens,\n",
    "                    0,\n",
    "                    (acc, y) -> IF(y = x, acc + 1, acc)\n",
    "                )\n",
    "            )\n",
    "            \"\"\"\n",
    "        )  # Count the frequencies of each token\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"Map\",\n",
    "        F.arrays_zip(\"Dist\", \"Counts\")  # Combine tokens and their counts into an array of structs\n",
    "    )\n",
    "    .drop(\"Dist\", \"Counts\")  # Drop intermediate columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfafd8c-9609-4f0a-ae15-edaf2d24c044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Add sorted map, most frequent, and second most frequent tokens\n",
    "freq_df = temp.withColumn(\n",
    "    \"SortedMap\",  # Sort the Map by counts in descending order\n",
    "    F.expr(\n",
    "        \"\"\"\n",
    "        array_sort(\n",
    "            Map,\n",
    "            (first, second) -> CASE WHEN first['Counts'] > second['Counts'] THEN -1 ELSE 1 END\n",
    "        )\n",
    "        \"\"\"\n",
    "    )\n",
    ").withColumn(\n",
    "    \"mf_aa\",  # Most frequent token as a list\n",
    "    F.expr(\"array(SortedMap[0]['Dist'])\")\n",
    ").withColumn(\n",
    "    \"mf_aa_count\",  # Count of the most frequent token\n",
    "    F.expr(\"SortedMap[0]['Counts']\")\n",
    ").withColumn(\n",
    "    \"mf_aa_freq\",  # Frequency of the most frequent token\n",
    "    F.col(\"mf_aa_count\") / F.size(F.col(\"Tokens\"))  # Divide count by total number of tokens\n",
    ").drop('tokens','length','Map','SortedMap','mf_aa_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c18578b-d451-43ac-aa2e-02e566f47958",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6542f17-8669-4f12-b791-077b24b3ae08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "freq_pd = freq_df.toPandas()\n",
    "# Maybe to csv?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57299a0a-c155-410f-ac86-6b3c13999c15",
   "metadata": {},
   "source": [
    "### Histogram of most frequent tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6f485f-d588-48a3-9622-1176845719fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_pd['mf_aa'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bea3e6-ffb6-4b54-8d33-0a4fd295a183",
   "metadata": {},
   "source": [
    "### Histogram of the frequency of most frequent tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57516e85-b906-4bd2-a8e8-88bddba65292",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_pd['mf_aa_freq'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b78c37-1fc4-49ea-88bd-7115c12075c4",
   "metadata": {},
   "source": [
    "### Creating the .vec file using ProtBert\n",
    "\n",
    "#### Here I shortly showed what I did in order to achieve the .vec file. Since the ProtBert model didn't have such file I looped through the residues and created the vocabulary. \n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "#transformer model for embedding space creation\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False ) # change model and tokenizer to t5\n",
    "model_embedd = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n",
    "\n",
    "vocab = ['L','A','G','V','E','S','I','K','R','D','T','P','N','Q','F','Y','M','H','C','W','X','U','B','Z','O']\n",
    "# Path to save the .vec file\n",
    "vec_file_path = 'prot_bert.vec'\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(vec_file_path, 'w') as f:\n",
    "    # Write the header (vocab size and vector dimension)\n",
    "    f.write(f\"{len(vocab)} {outputs.last_hidden_state.size()[2]}\\n\")\n",
    "    \n",
    "    # Write each word and its corresponding vector\n",
    "    for letter in vocab:\n",
    "        encoded_input = tokenizer.encode(letter,return_tensors='pt').to(device)\n",
    "        outputs = model_embedd(input_ids=encoded_input)\n",
    "        vector = outputs.last_hidden_state[0,0].detach().numpy()\n",
    "        vector_str = ' '.join(map(str, vector))  # Convert the vector to a string\n",
    "        f.write(f\"{letter} {vector_str}\\n\")\n",
    "```\n",
    "\n",
    "#### The problem with this implementation is that all the semantic information between the residues in a sequence is lost since the .vec file is created for each residue. This is highly affecting our output since this semantic knowledge contain ancestoral, functional, structural and many more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47d80f9-3b93-40c2-a025-7dc434951990",
   "metadata": {},
   "source": [
    "### Loading the embedding pre_trained model\n",
    "\n",
    "#### We loaded the .vec file as a standard python dictionary which is at first only available in the driver node. But using the Pyspark's broadcast function this dictionary can be broadcasted to all of the nodes which enables the executors to have the dictionaries in their local environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8911f26-292c-401d-bfdf-b24db40f27bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dictionary from the .vec file\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        letter_token = line.rstrip().split()\n",
    "        data[letter_token[0]] = DenseVector([float(letter) for letter in letter_token[1:]])\n",
    "    return data\n",
    "\n",
    "vec_dict = load_vectors('/data_files/prot_bert.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f4f32d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting the dictionary\n",
    "vec_broadcast = sc.broadcast(vec_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf9d5d4-4004-4ee6-b0b9-e75bc7731e30",
   "metadata": {},
   "source": [
    "### Creating list of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a58f139-a70e-4f4a-980c-45f8ed2da280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding the sequence\n",
    "# The embedding is done using a UDF an alternative idea might be by using a dataframe and giving conditions ?\n",
    "@F.udf(ArrayType(VectorUDT()))\n",
    "def embed_sequence(tokens_list):\n",
    "    return [vec_broadcast.value[token] for token in tokens_list if token in vec_dict]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49fa4d1e-bcec-4fbd-b7b4-f1b1869f8411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The embeddings are added a new column (UDF applied to the DataFrame)\n",
    "tokens_df = tokens_df.withColumn(\"embeddings\",embed_sequence(tokens_df.tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6ad938-f91f-4aa8-b9f1-69c3d117bec0",
   "metadata": {},
   "source": [
    "#### Output check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259ee491-fe98-4d92-a8f3-c6cdac2e1d2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokens_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142fb1ac-8833-4726-a736-ba5913877db6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_modified.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a1109f-c220-41e9-80be-318759647139",
   "metadata": {},
   "source": [
    "### Taking the mean of the list of the vectors (len(sequence) x 1024) to achieve one vector with length 1024\n",
    "\n",
    "##### To reduce the dimensions of the embeddings we used the same aproach as Rostlab did in order to achieve an embedding for the protein sequence.\n",
    "$$\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8a6634a-e0de-4cb2-9963-4499b602d05d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating another udf to get the mean of each embedding row\n",
    "@F.udf(VectorUDT())\n",
    "def mean_calculator(embedding,length):\n",
    "    mean_embedding = sum(embedding)/length\n",
    "    return mean_embedding   \n",
    "\n",
    "# Created mean embedding is added as a new column\n",
    "tokens_df = tokens_df.withColumn(\"mean_embed\",mean_calculator(tokens_df.embeddings,tokens_df.length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1c9cc88-3b24-4e26-9c56-ca0f8077bff2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job 2 cancelled \n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2731)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3013)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m mean_embed_rdd \u001b[38;5;241m=\u001b[39m tokens_df\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_embed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mrdd\n\u001b[1;32m      3\u001b[0m mean_embed_rdd \u001b[38;5;241m=\u001b[39m mean_embed_rdd\u001b[38;5;241m.\u001b[39mpersist() \u001b[38;5;66;03m# Persisting to get the data quickly since the cosine similarity is done on this dataframe\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmean_embed_rdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/rdd.py:2855\u001b[0m, in \u001b[0;36mRDD.take\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   2852\u001b[0m         taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2854\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned \u001b[38;5;241m+\u001b[39m numPartsToTry, totalParts))\n\u001b[0;32m-> 2855\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2857\u001b[0m items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m   2858\u001b[0m partsScanned \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m numPartsToTry\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/context.py:2510\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   2508\u001b[0m mappedRDD \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[1;32m   2509\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2510\u001b[0m sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmappedRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2511\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job 2 cancelled \n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2731)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3013)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "# Selecting a subset of the tokens_df in order to persist in the memory for future usage\n",
    "mean_embed_rdd = tokens_df.select(\"id\",\"mean_embed\").rdd\n",
    "mean_embed_rdd = mean_embed_rdd.persist() # Persisting to get the data quickly since the cosine similarity is done on this dataframe\n",
    "mean_embed_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea200acb-0cf2-4acf-8c9d-1875d55fff63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/27 11:51:02 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 3) (10.67.22.219 executor 2): TaskKilled (Stage cancelled: Job 2 cancelled )\n"
     ]
    }
   ],
   "source": [
    "# Creating a DF using the persisted RDD\n",
    "mean_embed_df = mean_embed_rdd.toDF()\n",
    "#mean_embed_df.printSchema(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81840f1-b98e-48c4-974e-e399098300c7",
   "metadata": {},
   "source": [
    "#### Checking output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0a51cf-d017-48a1-8fc0-3dc878f2628c",
   "metadata": {},
   "outputs": [],
   "source": [
    "take5 = mean_embed_df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2691f950-ccfd-426e-b0b1-3c2aa7fab3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(take5[3][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88adaf2f-4bc6-43ee-a773-7657b1b9c2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(take5[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde1ceab-e196-4688-ad1d-af989e22c95d",
   "metadata": {},
   "source": [
    "### Cosine similarity\n",
    "\n",
    "#### Here we created another udf for the calculation of the cosine similarity for each protein sequence. With this we can see if there are any similarities between protein sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e9a48f-44fb-4311-b3bd-0f8985cc75b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim_local(a, b):\n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477be641-1112-430c-a210-f1ab25ab160b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cartesian_rdd = mean_embed_rdd.cartesian(mean_embed_rdd)\n",
    "upper_triangle_rdd = cartesian_rdd.filter(lambda x: x[0][0] < x[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f052a3a-8c2a-46d6-b600-8f9fe82f7484",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rdd = upper_triangle_rdd.map(\n",
    "    lambda pair: (pair[0][0], pair[1][0], cos_sim_local(pair[0][1], pair[1][1]))\n",
    ")\n",
    "\n",
    "# Convert to DataFrame\n",
    "result_df = spark.createDataFrame(result_rdd, [\"row_idx1\", \"row_idx2\", \"dot_product\"])\n",
    "result_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54de871-6d19-4d54-b806-a0065f04b86b",
   "metadata": {},
   "source": [
    "I don't know if it's necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53765b62-240d-487f-be67-94029609ccb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/data_files/output_files/' \n",
    "result_df.repartition('row_idx1').write.partitionBy('row_idx1').csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e241ab-5337-4759-811e-d83a405e39b7",
   "metadata": {},
   "source": [
    "Go through each file and find the cos sim less than 98 collect the pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c14569-97f4-4ea9-a697-d3c5981f7fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_partitions = 100  # Adjust based on the size of your dataset and cluster resources\n",
    "repartitioned_df = result_df.repartition('row_idx1')\n",
    "filtered_df = repartitioned_df.filter(result_df[\"dot_product\"] < 0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f765708-29bd-45d2-b700-f7c94177fa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d128a2-af05-444d-9d93-2feb4e48c8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df2 = repartitioned_df.filter(result_df[\"dot_product\"] < 0.95)\n",
    "df2 = filtered_df2.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfb742b-58c4-41a0-acf1-0c439f4aec22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1556d14-a584-43e7-99da-0a6e4cc8b92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ec4c06-ea01-460c-93e8-c1a83a1bcaeb",
   "metadata": {},
   "source": [
    "#### For heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dd27d2-38d6-4f32-bf72-c4d5136674bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c82e58-6ddd-490f-a384-49afd4134739",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cos_df = result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1408bba5-1bad-41c1-b7f4-2f11a56a7af1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cos_df = df2_matrix.sort_values('row_idx1')\n",
    "\n",
    "#cod_df = cos_df.set_index('id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c99f87-cff3-481f-b895-c91eb94548ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_df.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b93676d-4d56-412a-a7ae-a4f6c75fc5b5",
   "metadata": {},
   "source": [
    "#### Heatmap trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d243e2-2df9-4ba1-87c4-ba5a1334e02e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "data2 = df2_matrix.values.flatten().tolist()\n",
    "data2_array = np.array(data2).reshape(len(df2_matrix.columns),len(df2_matrix.columns))\n",
    "sns.heatmap(data2_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20831fef-fa48-44ba-896b-d94a9060fe48",
   "metadata": {},
   "source": [
    "Here we see that the embeddings look very similar. I will continue the invastigation by checking the distribution of amino acids in the most similar and least similar sequence couples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f361d7e-7e86-4c1a-91ee-05e995b1e7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_embed_rdd.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc43de77-fb57-45a5-964a-017ff8216382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8396bcc2-5877-40c0-80ba-608205c04aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd7678b-5720-4d3b-8e15-9e1385d7263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35601b9c-e6c6-402c-80b3-e43be9e9f008",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
