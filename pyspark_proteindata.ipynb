{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0221e933-ac53-4274-baef-0600c23f09e9",
   "metadata": {},
   "source": [
    "### Embedding and Cosine Similarity\n",
    "\n",
    "In this section we are collecting the sequences to embed them using a Pretrained model from Rostlab's [ProtTrans](https://github.com/agemagician/ProtTrans). After this embedding the cosine similarity of each embedded protein sequence is achieved and graphed as a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00b87ec7-2990-4bb8-a637-1c5c6f7e3a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BioPython library for collecting the sequences from cif files\n",
    "from Bio.PDB import PDBList\n",
    "from Bio.PDB.MMCIFParser import MMCIFParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69cf507f-7791-4b9b-899b-0433a31fa609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation libraries\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "from collections import Counter\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import ArrayType, DoubleType, MapType\n",
    "from pyspark.ml.linalg import DenseVector, VectorUDT\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "700fe0d1-c9ef-41b3-b363-961b9fe463b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/07 22:43:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Creating the spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://master:7077\")\\\n",
    "    .appName(\"Proteindata spark application\")\\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\")\\\n",
    "    .config(\"spark.driver.host\", \"10.67.22.219\") \\\n",
    "    .config(\"spark.driver.port\",\"6066\")\\\n",
    "    .config(\"spark.blockManager.port\",\"7088\")\\\n",
    "    .config(\"spark.executor.memory\", \"3g\")\\\n",
    "    .config(\"spark.executor.cores\",\"4\")\\\n",
    "    .config(\"spark.sql.optimizer.enableRangeJoin\", \"true\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c1c570a-199d-4c57-b6d7-2173f3c763aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the spark context\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0654db69-75b0-466d-b46b-a9cdfb0fa4ae",
   "metadata": {},
   "source": [
    "### Getting the file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66a6ea2c-43a1-4fcd-be8c-5cc26a6df9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the file paths\n",
    "base_path = '/data_files/pdb_files'\n",
    "base_path_edit = '/data_files/pdb_files/{}'\n",
    "file_names = os.listdir(base_path)\n",
    "file_list = [base_path_edit.format(i) for i in file_names]#[:200]\n",
    "files_rdd = sc.parallelize(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c16b181-ef99-4972-8f03-188894c60d34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1712"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting all of the paths to see if there are any errors\n",
    "files_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40ea865f-353a-4265-86ec-233b9c6c2aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#files_rdd.coalesce(12)\n",
    "files_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cafcc4-1a5b-49ec-8425-b2524ebfe077",
   "metadata": {},
   "source": [
    "### Parsing the file\n",
    "\n",
    "#### Parsing the files using the Biopython library to get the sequence. The output of this function is the id of the protein, sequence and the length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de1a7fe6-05a7-487c-a79c-0892a18fd302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file(file):\n",
    "\n",
    "    cif_parser = MMCIFParser(QUIET=True) # CIF file parser\n",
    "    length = 0 # Setting the length initially to 0 for error correction\n",
    "    name = file.split('/')[3].split('.')[0] # Getting the id of the protein\n",
    "    structure = cif_parser.get_structure(\"protein\", file) # getting structure ? try \"protein\"\n",
    "\n",
    "    # Dictionary for residue names\n",
    "    d3to1 = {'CYS': 'C', 'ASP': 'D', 'SER': 'S', 'GLN': 'Q', 'LYS': 'K',\n",
    "    'ILE': 'I', 'PRO': 'P', 'THR': 'T', 'PHE': 'F', 'ASN': 'N',\n",
    "    'GLY': 'G', 'HIS': 'H', 'LEU': 'L', 'ARG': 'R', 'TRP': 'W',\n",
    "    'ALA': 'A', 'VAL':'V', 'GLU': 'E', 'TYR': 'Y', 'MET': 'M'}\n",
    "\n",
    "    # The output of the cif parser needs to be looped in order to get the sequence itself    \n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            sequence = [d3to1.get(residue.get_resname(), 'X') for residue in chain.get_residues()]\n",
    "            length = len(sequence)\n",
    "    \n",
    "    return name,sequence,length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6121fa2a-1c87-4f9c-9ef6-8658be829e52",
   "metadata": {},
   "source": [
    "### Creating a dataframe \n",
    "#### A dataframe containing the id, length and the token of the each sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d984795-af10-4d5e-ba73-a4859a88a078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an RDD for the tokens\n",
    "def tokens_df_creator(file_path):\n",
    "\n",
    "    data = []\n",
    "    name, sequence, length = parse_file(file_path)\n",
    "    row_value = {\n",
    "        'id':name,\n",
    "        'length':length,\n",
    "        'tokens':sequence,\n",
    "    }\n",
    "    #if 32 <= row_value['length'] <= 256:\n",
    "    data.append(Row(**row_value))\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Turning the RDD into a DF for easier usage\n",
    "tokens_rdd = files_rdd.flatMap(tokens_df_creator) # FlatMap applied to the RDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a0cb23b-e607-4e9c-8a18-0ddbe635d86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tokens_df = tokens_rdd.toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92198f8-0468-448d-98b9-98071c3a23cc",
   "metadata": {},
   "source": [
    "#### Checking output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d70ed89-ee64-4a40-aaa0-7b1f83c34f4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokens_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232aae94-6aca-495f-af07-00184aebea12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Checking if there is any missing data\n",
    "tokens_df.where(tokens_df.length==0).show()\n",
    "# running twice showString at NativeMethodAccessorImpl.java:0\n",
    "#8 minutes\n",
    "#12 minutes ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018b4aad-11d7-4610-9f5b-dd0a43aa25e8",
   "metadata": {},
   "source": [
    "### Frequency analysis for every sequence\n",
    "\n",
    "#### Here I am going to create a dataframe consisting of the frequency analysis of each sequence. The dataframe will consist of columns as, sequence id, most frequent amino acid (mf_aa), most frequent amino acid percentage (mf_aa_freq)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ecb63d-aa53-4459-b8f5-e155b1060eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a10a082-bc08-4ccc-9624-3fa22bb0a043",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = (\n",
    "    tokens_df\n",
    "    .withColumn(\"Dist\", F.array_distinct(\"Tokens\"))  # Get distinct tokens for the current sequence\n",
    "    .withColumn(\n",
    "        \"Counts\",\n",
    "        F.expr(\n",
    "            \"\"\"\n",
    "            transform(\n",
    "                Dist,\n",
    "                x -> aggregate(\n",
    "                    Tokens,\n",
    "                    0,\n",
    "                    (acc, y) -> IF(y = x, acc + 1, acc)\n",
    "                )\n",
    "            )\n",
    "            \"\"\"\n",
    "        )  # Count the frequencies of each token\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"Map\",\n",
    "        F.arrays_zip(\"Dist\", \"Counts\")  # Combine tokens and their counts into an array of structs\n",
    "    )\n",
    "    .drop(\"Dist\", \"Counts\")  # Drop intermediate columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfafd8c-9609-4f0a-ae15-edaf2d24c044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Add sorted map, most frequent, and second most frequent tokens\n",
    "freq_df = temp.withColumn(\n",
    "    \"SortedMap\",  # Sort the Map by counts in descending order\n",
    "    F.expr(\n",
    "        \"\"\"\n",
    "        array_sort(\n",
    "            Map,\n",
    "            (first, second) -> CASE WHEN first['Counts'] > second['Counts'] THEN -1 ELSE 1 END\n",
    "        )\n",
    "        \"\"\"\n",
    "    )\n",
    ").withColumn(\n",
    "    \"mf_aa\",  # Most frequent token as a list\n",
    "    F.expr(\"array(SortedMap[0]['Dist'])\")\n",
    ").withColumn(\n",
    "    \"mf_aa_count\",  # Count of the most frequent token\n",
    "    F.expr(\"SortedMap[0]['Counts']\")\n",
    ").withColumn(\n",
    "    \"mf_aa_freq\",  # Frequency of the most frequent token\n",
    "    F.col(\"mf_aa_count\") / F.size(F.col(\"Tokens\"))  # Divide count by total number of tokens\n",
    ").drop('tokens','length','Map','SortedMap','mf_aa_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c18578b-d451-43ac-aa2e-02e566f47958",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6542f17-8669-4f12-b791-077b24b3ae08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "freq_pd = freq_df.toPandas()\n",
    "# Maybe to csv?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57299a0a-c155-410f-ac86-6b3c13999c15",
   "metadata": {},
   "source": [
    "### Histogram of most frequent tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6f485f-d588-48a3-9622-1176845719fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df['mf_aa'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bea3e6-ffb6-4b54-8d33-0a4fd295a183",
   "metadata": {},
   "source": [
    "### Histogram of the frequency of most frequent tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57516e85-b906-4bd2-a8e8-88bddba65292",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_pd['mf_aa_freq'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f5065a-de88-4293-ac99-fe358547032a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(freq_pd['mf_aa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba04f215-85ef-49f1-8dc1-0d872941a736",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.bar(freq_pd['mf_aa_freq'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b78c37-1fc4-49ea-88bd-7115c12075c4",
   "metadata": {},
   "source": [
    "### Creating the .vec file using ProtBert\n",
    "\n",
    "#### Here I shortly showed what I did in order to achieve the .vec file. Since the ProtBert model didn't have such file I looped through the residues and created the vocabulary. \n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "#transformer model for embedding space creation\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False ) # change model and tokenizer to t5\n",
    "model_embedd = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n",
    "\n",
    "vocab = ['L','A','G','V','E','S','I','K','R','D','T','P','N','Q','F','Y','M','H','C','W','X','U','B','Z','O']\n",
    "# Path to save the .vec file\n",
    "vec_file_path = 'prot_bert.vec'\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(vec_file_path, 'w') as f:\n",
    "    # Write the header (vocab size and vector dimension)\n",
    "    f.write(f\"{len(vocab)} {outputs.last_hidden_state.size()[2]}\\n\")\n",
    "    \n",
    "    # Write each word and its corresponding vector\n",
    "    for letter in vocab:\n",
    "        encoded_input = tokenizer.encode(letter,return_tensors='pt').to(device)\n",
    "        outputs = model_embedd(input_ids=encoded_input)\n",
    "        vector = outputs.last_hidden_state[0,0].detach().numpy()\n",
    "        vector_str = ' '.join(map(str, vector))  # Convert the vector to a string\n",
    "        f.write(f\"{letter} {vector_str}\\n\")\n",
    "```\n",
    "\n",
    "#### The problem with this implementation is that all the semantic information between the residues in a sequence is lost since the .vec file is created for each residue. This is highly affecting our output since this semantic knowledge contain ancestoral, functional, structural and many more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47d80f9-3b93-40c2-a025-7dc434951990",
   "metadata": {},
   "source": [
    "### Loading the embedding pre_trained model\n",
    "\n",
    "#### We loaded the .vec file as a standard python dictionary which is at first only available in the driver node. But using the Pyspark's broadcast function this dictionary can be broadcasted to all of the nodes which enables the executors to have the dictionaries in their local environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8911f26-292c-401d-bfdf-b24db40f27bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dictionary from the .vec file\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        letter_token = line.rstrip().split()\n",
    "        data[letter_token[0]] = DenseVector([float(letter) for letter in letter_token[1:]])\n",
    "    return data\n",
    "\n",
    "vec_dict = load_vectors('/data_files/prot_bert.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f4f32d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting the dictionary\n",
    "vec_broadcast = sc.broadcast(vec_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf9d5d4-4004-4ee6-b0b9-e75bc7731e30",
   "metadata": {},
   "source": [
    "### Creating list of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a58f139-a70e-4f4a-980c-45f8ed2da280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding the sequence\n",
    "# The embedding is done using a UDF an alternative idea might be by using a dataframe and giving conditions ?\n",
    "@F.udf(ArrayType(VectorUDT()))\n",
    "def embed_sequence(tokens_list):\n",
    "    return [vec_broadcast.value[token] for token in tokens_list if token in vec_dict]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49fa4d1e-bcec-4fbd-b7b4-f1b1869f8411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The embeddings are added a new column (UDF applied to the DataFrame)\n",
    "tokens_df = tokens_df.withColumn(\"embeddings\",embed_sequence(tokens_df.tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6ad938-f91f-4aa8-b9f1-69c3d117bec0",
   "metadata": {},
   "source": [
    "#### Output check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259ee491-fe98-4d92-a8f3-c6cdac2e1d2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokens_df.take(1)\n",
    "#  mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142fb1ac-8833-4726-a736-ba5913877db6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#freq_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a1109f-c220-41e9-80be-318759647139",
   "metadata": {},
   "source": [
    "### Taking the mean of the list of the vectors (len(sequence) x 1024) to achieve one vector with length 1024\n",
    "\n",
    "##### To reduce the dimensions of the embeddings we used the same aproach as Rostlab did, in order to achieve an embedding for the protein sequence.\n",
    "$$\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8a6634a-e0de-4cb2-9963-4499b602d05d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating another udf to get the mean of each embedding row\n",
    "@F.udf(VectorUDT())\n",
    "def mean_calculator(embedding,length):\n",
    "    mean_embedding = sum(embedding)/length\n",
    "    return mean_embedding   \n",
    "\n",
    "# Created mean embedding is added as a new column\n",
    "tokens_df = tokens_df.withColumn(\"mean_embed\",mean_calculator(tokens_df.embeddings,tokens_df.length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1c9cc88-3b24-4e26-9c56-ca0f8077bff2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Selecting a subset of the tokens_df in order to persist in the memory for future usage\n",
    "mean_embed_df = tokens_df.select(\"id\",\"mean_embed\")\n",
    "#mean_embed_rdd = tokens_df.select(\"id\",\"mean_embed\").rdd\n",
    "#mean_embed_rdd = mean_embed_rdd.persist() # Persisting to get the data quickly since the cosine similarity is done on this dataframe\n",
    "#mean_embed_df.take(3)\n",
    "# 18 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81840f1-b98e-48c4-974e-e399098300c7",
   "metadata": {},
   "source": [
    "#### Checking output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0a51cf-d017-48a1-8fc0-3dc878f2628c",
   "metadata": {},
   "outputs": [],
   "source": [
    "take5 = mean_embed_df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2691f950-ccfd-426e-b0b1-3c2aa7fab3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(take5[3][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88adaf2f-4bc6-43ee-a773-7657b1b9c2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(take5[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde1ceab-e196-4688-ad1d-af989e22c95d",
   "metadata": {},
   "source": [
    "### Cosine similarity\n",
    "\n",
    "#### Here we created another udf for the calculation of the cosine similarity for each protein sequence. With this we can see if there are any similarities between protein sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eecae3-bf8a-4544-81fa-a630cf53370f",
   "metadata": {},
   "source": [
    "### Large amounts of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b92f7e3d-b478-4921-9b08-c5343107f216",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_embed_df.withColumnRenamed('id', 'id1').createOrReplaceTempView(\"df1\")\n",
    "\n",
    "mean_embed_df.withColumnRenamed('id', 'id2').createOrReplaceTempView(\"df2\")\n",
    "\n",
    "joined_df = spark.sql(\n",
    "\"\"\"SELECT *\n",
    "FROM df1, df2\n",
    "WHERE df1.id1 < df2.id2\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5baa4123-99b3-4863-89d3-b3f3a54d1592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim_batch(iterator):\n",
    "    import numpy as np\n",
    "    return [(x[0], x[2], float(np.dot(x[1], x[3]) / (np.linalg.norm(x[1]) * np.linalg.norm(x[3]))))\n",
    "            for x in iterator]\n",
    "\n",
    "result_rdd = joined_df.rdd.mapPartitions(cos_sim_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa3fb39-8b20-415c-8799-b1341fda2153",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2875727e-da0f-44de-b1ba-a45f89e085f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "\n",
    "filtered_df = spark.createDataFrame(result_rdd, [\"id1\", \"id2\", \"cos_sim\"]).filter(\"cos_sim < 0.955\")\n",
    "#result_df.take(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f765708-29bd-45d2-b700-f7c94177fa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adce6ec3-6a89-4148-9f24-260662e14efd",
   "metadata": {},
   "source": [
    "Getting the unique ids to create the heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eb403b-ce62-4fbf-b1a9-3f52d908c8da",
   "metadata": {},
   "source": [
    "#### Creating the matrix for creating the heatmap easily with filtered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d97c28-0f29-4a7e-aaec-791c69817936",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_idx1 = filtered_df.select('id1').distinct().rdd.map(lambda r: r[0]).collect()\n",
    "row_idx2 = filtered_df.select('id2').distinct().rdd.map(lambda r: r[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecd4bcf-9d0f-402b-86e8-cc6dda71d2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = list(set(row_idx1 + row_idx2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896cb189-0e05-4ccc-9e50-9f84c5562105",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_mean_embed_df = mean_embed_df.filter(mean_embed_df[\"id\"].isin(idx) == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eebd45d-066c-4259-a61e-7e450697807f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered_mean_embed_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeba417-f61a-4c60-acf9-203e54eb5487",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = (\n",
    "    filtered_mean_embed_df.alias('a')\n",
    "    .crossJoin(filtered_mean_embed_df.alias('b'))\n",
    "    .withColumn(\n",
    "        'cs',\n",
    "        cos_sim(\n",
    "            'a.mean_embed',\n",
    "            'b.mean_embed')\n",
    "    )\n",
    "    .groupby('a.id') # Grouping by the id's\n",
    "    .pivot('b.id') # Pivoting on the same id's\n",
    "    .sum('cs') # Collecting the cosine similarities\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ec4c06-ea01-460c-93e8-c1a83a1bcaeb",
   "metadata": {},
   "source": [
    "#### For heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dd27d2-38d6-4f32-bf72-c4d5136674bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cos_df = df2.toPandas()\n",
    "cos_df = cos_df.sort_values('id')\n",
    "\n",
    "cod_df = cos_df.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9529bec-e852-465f-aa1d-1412db54c0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cod_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b93676d-4d56-412a-a7ae-a4f6c75fc5b5",
   "metadata": {},
   "source": [
    "#### Heatmap trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d243e2-2df9-4ba1-87c4-ba5a1334e02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "data2 = cod_df.values.flatten().tolist()\n",
    "data2_array = np.array(data2).reshape(len(cod_df.columns),len(cod_df.columns))\n",
    "sns.heatmap(data2_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20831fef-fa48-44ba-896b-d94a9060fe48",
   "metadata": {},
   "source": [
    "Here we see that the embeddings look very similar. I will continue the invastigation by checking the distribution of amino acids in the most similar and least similar sequence couples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a66bb8c-3532-465a-8651-e6f4e540f2d7",
   "metadata": {},
   "source": [
    "### Analysis of some proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9e1c74-cbac-4168-a506-c92c31a81d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_cs = filtered_df.select(F.min(filtered_df[\"dot_product\"])).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e22c3ff-db44-4328-bbd2-4f1d8b776464",
   "metadata": {},
   "outputs": [],
   "source": [
    "proteins = filtered_df.filter(filtered_df[\"dot_product\"] == minimum_cs[0][0]).collect()\n",
    "protein1, protein2 = proteins[0][0],proteins[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fad752-892a-4a2d-95c1-b3557dd39307",
   "metadata": {},
   "outputs": [],
   "source": [
    "protein1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92d296b-380a-4c3c-b5bd-e4fc65a29e60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ece3e65-abd8-4c41-95cc-d233bb4a928f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f361d7e-7e86-4c1a-91ee-05e995b1e7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_embed_rdd.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc43de77-fb57-45a5-964a-017ff8216382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8396bcc2-5877-40c0-80ba-608205c04aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd7678b-5720-4d3b-8e15-9e1385d7263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35601b9c-e6c6-402c-80b3-e43be9e9f008",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
