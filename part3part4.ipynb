{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#phi and psi\n",
    "from Bio.PDB.MMCIFParser import MMCIFParser\n",
    "from Bio import PDB\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "# import the python libraries to create/connect to a Spark Session\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import MapType, ArrayType, StringType, FloatType#, IntegerType, FloatType, StringType\n",
    "from pyspark.sql.functions import udf, pandas_udf, col\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "from pyspark.ml.linalg import VectorUDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://master:7077\")\\\n",
    "    .appName(\"Proteindata spark application\")\\\n",
    "    .config(\"spark.executor.memory\", \"4096m\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark)\n",
    "# create a spark context\n",
    "sc = spark.sparkContext\n",
    "# print its status\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/data_files/cif_files'\n",
    "base_path_edit = '/data_files/cif_files/{}'\n",
    "file_names = os.listdir(base_path)\n",
    "#file_list = [base_path_edit.format(i) for i in file_names]]\n",
    "small_file_list = [base_path_edit.format(i) for i in file_names][:129]\n",
    "#files_rdd = sc.parallelize(file_list)\n",
    "small_files_rdd = sc.parallelize(small_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_files_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file(file):\n",
    "    #keys = names\n",
    "    sequence = str()\n",
    "    name = str()\n",
    "    length = int()\n",
    "    with open(file, 'r') as fin:\n",
    "        lines = fin.readlines()\n",
    "           \n",
    "    for i,line in enumerate(lines):\n",
    "        #Cleaning the end\n",
    "        line.replace('\\n','')\n",
    "        #Getting the id\n",
    "        id_line = re.findall(r'^_entry.id.*',line)\n",
    "        if len(id_line) != 0:\n",
    "            name = line.replace(' ','').split('id')[1].replace('\\n','')\n",
    "\n",
    "        #Getting the sequence\n",
    "        seq_line = re.findall(r'^_entity_poly\\.pdbx_seq_one_letter_code_can\\s{3}.*',line)\n",
    "        if len(seq_line) != 0:\n",
    "            seq_line_0 = re.findall(r'^_entity_poly\\.pdbx_seq_one_letter_code_can\\s{3}\\S.*',line)\n",
    "            if len(seq_line_0) != 0:\n",
    "                sequence = seq_line[0].split('can')[1].replace('\\n','').replace(' ','')\n",
    "            else:\n",
    "                sequence = lines[i+1].split(' ')[0].replace(';','').replace('\\n','').replace(' ','')\n",
    "                if ';' not in lines[i+2]:\n",
    "                    sequence = sequence + lines[i+2].split(' ')[0].replace('\\n','').replace(' ','')\n",
    "                elif ';' not in lines[i+3]:\n",
    "                    sequence = sequence + lines[i+3].split(' ')[0].replace('\\n','').replace(' ','')\n",
    "                    \n",
    "        length = len(sequence)\n",
    "        \n",
    "    return name,sequence,length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(l, dtype=float):  \n",
    "    return list(map(dtype, l))\n",
    "\n",
    "def angle_transformer(file_path):\n",
    "    file_model = file_path.split(\".\")[0]\n",
    "    cif_parser = MMCIFParser()\n",
    "    structure = cif_parser.get_structure(file_model, file_path)\n",
    "    structure.atom_to_internal_coordinates() # turns xyz coordinates into angles and bond lengths\n",
    "    chain:PDB.Chain.Chain = list(structure.get_chains())[0]#iterator of chains, turns it into list, [0] first chain\n",
    "\n",
    "    ic_chain: PDB.internal_coords.IC_Chain = chain.internal_coord #this access the internal chain coords of the chain object\n",
    "\n",
    "    d: Dict[Tuple[PDB.internal_coords.AtomKey,\n",
    "              PDB.internal_coords.AtomKey,\n",
    "              PDB.internal_coords.AtomKey,\n",
    "              PDB.internal_coords.AtomKey],\n",
    "        PDB.internal_coords.Dihedron] = ic_chain.dihedra\n",
    "\n",
    "    cnt = 1\n",
    "    phi_angles_list = []\n",
    "    psi_angles_list = []\n",
    "\n",
    "    for key in d:\n",
    "        if key[0].akl[3] == 'N' and key[1].akl[3] == 'CA' and key[2].akl[3] == 'C' and key[3].akl[3] == 'N':\n",
    "            phi_angles_list.append(d[key].angle)\n",
    "        elif key[0].akl[3] == 'CA' and key[1].akl[3] == 'C' and key[2].akl[3] == 'N' and key[3].akl[3] == 'CA':\n",
    "            psi_angles_list.append(d[key].angle)\n",
    "\n",
    "    \n",
    "    \n",
    "    psi_angles_list.append(0)\n",
    "    psi_angles_list = foo(psi_angles_list)\n",
    "    #psi = np.asarray(psi_angles_list,dtype=np.float32)*(np.pi/180)\n",
    "\n",
    "    phi_angles_list.append(0)\n",
    "    phi_angles_list = foo(phi_angles_list)\n",
    "    #phi = np.asarray(phi_angles_list,dtype=np.float32)*(np.pi/180)\n",
    "    return phi_angles_list,psi_angles_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_aa(sequence):\n",
    "    sequence = ' '.join(sequence)\n",
    "    aa_s = sequence.split(' ')\n",
    "    return aa_s\n",
    "\n",
    "def tokens_df_creator(file_path):\n",
    "\n",
    "    data = []\n",
    "    name, sequence, length = parse_file(file_path)\n",
    "    row_value = {\n",
    "        'id':name,\n",
    "        'length':length,\n",
    "        'tokens':split_aa(sequence),\n",
    "    }\n",
    "    if row_value['length'] <= 128:\n",
    "        data.append(Row(**row_value))\n",
    "    \n",
    "    return data\n",
    "\n",
    "tokens_rdd = sc.parallelize(small_file_list)\\\n",
    "        .flatMap(tokens_df_creator)\n",
    "\n",
    "\n",
    "\n",
    "def angles_df_creator(file_path):\n",
    "\n",
    "    data = []\n",
    "    name, sequence, length = parse_file(file_path)\n",
    "    phi, psi = angle_transformer(file_path)\n",
    "    row_value = {\n",
    "        'id':name,\n",
    "        'phi':Vectors.dense(phi),\n",
    "        'psi':Vectors.dense(psi)\n",
    "    }\n",
    "    if length <= 128:\n",
    "        data.append(Row(**row_value))\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "angles_rdd = sc.parallelize(small_file_list)\\\n",
    "        .flatMap(angles_df_creator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_df = tokens_rdd.toDF()\n",
    "angles_df = angles_rdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        letter_token = line.rstrip().split()\n",
    "        data[letter_token[0]] = [float(letter) for letter in letter_token[1:]]\n",
    "    return data\n",
    "\n",
    "vec_dict = load_vectors(\"/data_files/prot_bert.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_broadcast = sc.broadcast(vec_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(ArrayType(ArrayType(FloatType())))\n",
    "def embed_sequence(tokens):\n",
    "    if len(tokens) < 128:\n",
    "        local_vec_dict = vec_broadcast.value\n",
    "        tokens_list = tokens\n",
    "        padding = ['X' for i in range(128-len(tokens_list))]\n",
    "        tokens_list.extend(padding)\n",
    "    return [local_vec_dict[token] for token in tokens_list if token in vec_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_df =tokens_df.withColumn(\"embeddings\",embed_sequence(tokens_df.tokens))\n",
    "tokens_df.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_qr(embedding):\n",
    "    Q, R = np.linalg.qr(embedding)\n",
    "    return R\n",
    "    \n",
    "#need to collect R's and stack them up we can just \"append\" them maybe at the end?\n",
    "#maybe reduce with a function?\n",
    "\n",
    "def second_qr(R):\n",
    "    Q_til, R_til = np.linalg.qr(R)\n",
    "    return R_til\n",
    "#now I think it can just be collected\n",
    "\n",
    "def funcky(tokens):\n",
    "    \n",
    "    first_qr(embedding)\n",
    "\n",
    "embedding = embed_sequence(small_df.tokens)\n",
    "embedding_rdd = embedding.repartition() #create nxn in each partition\n",
    "\n",
    "r_rdd = embedding_rdd.map(first_qr).reduce(stack_them)\n",
    "r_global = r_rdd.reshufle().map(second_qr).collect()\n",
    "\n",
    "U, eps, V_T = r_global.svd()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
